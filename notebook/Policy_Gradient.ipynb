{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oQvd_lpUpTyw"
      },
      "source": [
        "Thanks to https://medium.com/swlh/cartpole-with-policy-gradient-tensorflow-2-x-3a7a14b9cc03"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "cBEAVGuPl-FK"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow.keras import Model\n",
        "import tensorflow_probability as tfp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "c30gXs5VmE_X"
      },
      "outputs": [],
      "source": [
        "class MakeModel(Model):\n",
        "    def __init__(self,num_actions):\n",
        "        super().__init__()\n",
        "        self.fc1 = tf.keras.layers.Dense(32,activation='relu')\n",
        "        self.fc2 = tf.keras.layers.Dense(32,activation='relu')\n",
        "        self.action = tf.keras.layers.Dense(num_actions,activation='softmax')\n",
        "        \n",
        "\n",
        "    def call(self,state):\n",
        "        x = tf.convert_to_tensor(state)\n",
        "        x = self.fc1(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.action(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "cUMqWwIQmL3w"
      },
      "outputs": [],
      "source": [
        "class Agent:\n",
        "    def __init__(self,gamma=0.95,lr=0.001,n_actions=2):\n",
        "        self.gamma = gamma\n",
        "        self.lr = lr\n",
        "        self.model = MakeModel(n_actions)\n",
        "        self.opt = tf.keras.optimizers.Adam(learning_rate=self.lr)\n",
        "        self.action_memory = []\n",
        "        self.reward_memory = []\n",
        "        self.state_memory = []\n",
        "\n",
        "    def choose_action(self,state):\n",
        "        prob = self.model(np.array([state]))\n",
        "        dist = tfp.distributions.Categorical(probs=prob,dtype=tf.float32)\n",
        "        action = dist.sample()\n",
        "        self.action_memory.append(action)\n",
        "        return int(action.numpy()[0])\n",
        "\n",
        "    def store_reward(self,reward):\n",
        "        self.reward_memory.append(reward)\n",
        "\n",
        "    def store_state(self,state):\n",
        "        self.state_memory.append(state)\n",
        "\n",
        "    def learn(self):\n",
        "        # G = np.zeros_like(self.reward_memory)\n",
        "        # for i in range(len(self.reward_memory)):\n",
        "        #     discount = 1\n",
        "        #     g_sum = 0\n",
        "        #     for j in range(i,len(self.reward_memory)):\n",
        "        #         g_sum += self.reward_memory[j] * discount\n",
        "        #         discount *= self.gamma\n",
        "        #     G[i] = g_sum\n",
        "        sum_reward = 0\n",
        "        discnt_rewards = []\n",
        "        self.reward_memory.reverse()\n",
        "        for r in self.reward_memory:\n",
        "            sum_reward = r + self.gamma*sum_reward\n",
        "            discnt_rewards.append(sum_reward)\n",
        "        discnt_rewards.reverse() \n",
        "        \n",
        "\n",
        "        for state,action,reward in zip(self.state_memory,self.action_memory,discnt_rewards):\n",
        "            with tf.GradientTape() as tape:\n",
        "                p = self.model(np.array([state]),training=True)\n",
        "                loss = self.calc_loss(p,action,reward)\n",
        "                grads = tape.gradient(loss,self.model.trainable_variables)\n",
        "                self.opt.apply_gradients(zip(grads,self.model.trainable_variables))\n",
        "\n",
        "        self.reward_memory = []\n",
        "        self.action_memory = []\n",
        "        self.state_memory = []\n",
        "\n",
        "    def calc_loss(self,prob,action,reward):\n",
        "        dist = tfp.distributions.Categorical(probs=prob, dtype=tf.float32)\n",
        "        log_prob = dist.log_prob(action)\n",
        "        loss = -log_prob*reward\n",
        "        return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "TNMYevwsmWni"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "score_arr = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6CIvAKS9mOJ2",
        "outputId": "5e2c90eb-4751-4d57-a91c-5426070a210e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Python310\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:97: UserWarning: \u001b[33mWARN: We recommend you to use a symmetric and normalized Box action space (range=[-1, 1]) https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html\u001b[0m\n",
            "  logger.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "episode done: 2\t score recieved: 25.0\n",
            "episode done: 3\t score recieved: 15.0\n",
            "episode done: 4\t score recieved: 22.0\n",
            "episode done: 5\t score recieved: 20.0\n",
            "episode done: 6\t score recieved: 11.0\n",
            "episode done: 7\t score recieved: 14.0\n",
            "episode done: 8\t score recieved: 10.0\n",
            "episode done: 9\t score recieved: 16.0\n",
            "episode done: 10\t score recieved: 16.0\n",
            "episode done: 11\t score recieved: 20.0\n",
            "episode done: 12\t score recieved: 12.0\n",
            "episode done: 13\t score recieved: 15.0\n",
            "episode done: 14\t score recieved: 25.0\n",
            "episode done: 15\t score recieved: 23.0\n",
            "episode done: 16\t score recieved: 11.0\n",
            "episode done: 17\t score recieved: 34.0\n",
            "episode done: 18\t score recieved: 20.0\n",
            "episode done: 19\t score recieved: 12.0\n",
            "episode done: 20\t score recieved: 12.0\n",
            "episode done: 21\t score recieved: 18.0\n",
            "episode done: 22\t score recieved: 66.0\n",
            "episode done: 23\t score recieved: 13.0\n",
            "episode done: 24\t score recieved: 15.0\n",
            "episode done: 25\t score recieved: 15.0\n",
            "episode done: 26\t score recieved: 9.0\n",
            "episode done: 27\t score recieved: 16.0\n",
            "episode done: 28\t score recieved: 9.0\n",
            "episode done: 29\t score recieved: 14.0\n",
            "episode done: 30\t score recieved: 26.0\n",
            "episode done: 31\t score recieved: 20.0\n",
            "episode done: 32\t score recieved: 20.0\n",
            "episode done: 33\t score recieved: 27.0\n",
            "episode done: 34\t score recieved: 13.0\n",
            "episode done: 35\t score recieved: 21.0\n",
            "episode done: 36\t score recieved: 21.0\n",
            "episode done: 37\t score recieved: 52.0\n",
            "episode done: 38\t score recieved: 15.0\n",
            "episode done: 39\t score recieved: 12.0\n",
            "episode done: 40\t score recieved: 66.0\n",
            "episode done: 41\t score recieved: 36.0\n",
            "episode done: 42\t score recieved: 14.0\n",
            "episode done: 43\t score recieved: 11.0\n",
            "episode done: 44\t score recieved: 60.0\n",
            "episode done: 45\t score recieved: 15.0\n",
            "episode done: 46\t score recieved: 27.0\n",
            "episode done: 47\t score recieved: 32.0\n",
            "episode done: 48\t score recieved: 38.0\n",
            "episode done: 49\t score recieved: 39.0\n",
            "episode done: 50\t score recieved: 42.0\n",
            "episode done: 52\t score recieved: 30.0\n",
            "episode done: 53\t score recieved: 80.0\n",
            "episode done: 54\t score recieved: 91.0\n",
            "episode done: 55\t score recieved: 139.0\n",
            "episode done: 56\t score recieved: 17.0\n",
            "episode done: 57\t score recieved: 42.0\n",
            "episode done: 58\t score recieved: 83.0\n",
            "episode done: 59\t score recieved: 101.0\n",
            "episode done: 60\t score recieved: 96.0\n",
            "episode done: 61\t score recieved: 71.0\n",
            "episode done: 62\t score recieved: 159.0\n",
            "episode done: 63\t score recieved: 110.0\n",
            "episode done: 64\t score recieved: 68.0\n",
            "episode done: 65\t score recieved: 234.0\n",
            "episode done: 66\t score recieved: 154.0\n",
            "episode done: 67\t score recieved: 19.0\n",
            "episode done: 68\t score recieved: 177.0\n",
            "episode done: 69\t score recieved: 125.0\n",
            "episode done: 70\t score recieved: 71.0\n",
            "episode done: 71\t score recieved: 78.0\n",
            "episode done: 72\t score recieved: 28.0\n",
            "episode done: 73\t score recieved: 104.0\n",
            "episode done: 74\t score recieved: 96.0\n",
            "episode done: 75\t score recieved: 87.0\n",
            "episode done: 76\t score recieved: 153.0\n",
            "episode done: 77\t score recieved: 153.0\n",
            "episode done: 78\t score recieved: 157.0\n",
            "episode done: 79\t score recieved: 134.0\n",
            "episode done: 80\t score recieved: 178.0\n",
            "episode done: 81\t score recieved: 62.0\n",
            "episode done: 82\t score recieved: 124.0\n",
            "episode done: 83\t score recieved: 144.0\n",
            "episode done: 84\t score recieved: 103.0\n",
            "episode done: 85\t score recieved: 164.0\n",
            "episode done: 86\t score recieved: 236.0\n",
            "episode done: 87\t score recieved: 162.0\n",
            "episode done: 88\t score recieved: 200.0\n",
            "episode done: 89\t score recieved: 78.0\n"
          ]
        }
      ],
      "source": [
        "env = gym.make('CartPole-v1')\n",
        "agent = Agent()\n",
        "num_episodes = 500\n",
        "\n",
        "for i in range(num_episodes):\n",
        "    state = env.reset()\n",
        "    score = 0\n",
        "    rewards = []\n",
        "    states = []\n",
        "    actions = []\n",
        "    done = False\n",
        "    while not done:\n",
        "        action = agent.choose_action(state)\n",
        "        state_,reward,done,_ = env.step(action)\n",
        "        agent.store_reward(reward)\n",
        "        agent.store_state(state)\n",
        "        state = state_\n",
        "        score += reward\n",
        "        # env.render()\n",
        "        if done:\n",
        "            agent.learn()\n",
        "            score_arr.append(score)\n",
        "            if i % 50:\n",
        "                print(f'episode done: {i+1}\\t score recieved: {score}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t-bjAkcBmSIm"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(score_arr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4 (tags/v3.10.4:9d38120, Mar 23 2022, 23:13:41) [MSC v.1929 64 bit (AMD64)]"
    },
    "vscode": {
      "interpreter": {
        "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
