{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oQvd_lpUpTyw"
      },
      "source": [
        "Thanks to https://medium.com/swlh/cartpole-with-policy-gradient-tensorflow-2-x-3a7a14b9cc03"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cBEAVGuPl-FK"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow.keras import Model\n",
        "import tensorflow_probability as tfp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c30gXs5VmE_X"
      },
      "outputs": [],
      "source": [
        "class MakeModel(Model):\n",
        "    def __init__(self,num_actions):\n",
        "        super().__init__()\n",
        "        self.fc1 = tf.keras.layers.Dense(32,activation='relu')\n",
        "        self.fc2 = tf.keras.layers.Dense(32,activation='relu')\n",
        "        self.action = tf.keras.layers.Dense(num_actions,activation='softmax')\n",
        "        \n",
        "\n",
        "    def call(self,state):\n",
        "        x = tf.convert_to_tensor(state)\n",
        "        x = self.fc1(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.action(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cUMqWwIQmL3w"
      },
      "outputs": [],
      "source": [
        "class Agent:\n",
        "    def __init__(self,gamma=0.95,lr=0.001,n_actions=2):\n",
        "        self.gamma = gamma\n",
        "        self.lr = lr\n",
        "        self.model = MakeModel(n_actions)\n",
        "        self.opt = tf.keras.optimizers.Adam(learning_rate=self.lr)\n",
        "        self.action_memory = []\n",
        "        self.reward_memory = []\n",
        "        self.state_memory = []\n",
        "\n",
        "    def choose_action(self,state):\n",
        "        prob = self.model(np.array([state]))\n",
        "        dist = tfp.distributions.Categorical(probs=prob,dtype=tf.float32)\n",
        "        action = dist.sample()\n",
        "        self.action_memory.append(action)\n",
        "        return int(action.numpy()[0])\n",
        "\n",
        "    def store_reward(self,reward):\n",
        "        self.reward_memory.append(reward)\n",
        "\n",
        "    def store_state(self,state):\n",
        "        self.state_memory.append(state)\n",
        "\n",
        "    def learn(self):\n",
        "        # G = np.zeros_like(self.reward_memory)\n",
        "        # for i in range(len(self.reward_memory)):\n",
        "        #     discount = 1\n",
        "        #     g_sum = 0\n",
        "        #     for j in range(i,len(self.reward_memory)):\n",
        "        #         g_sum += self.reward_memory[j] * discount\n",
        "        #         discount *= self.gamma\n",
        "        #     G[i] = g_sum\n",
        "        sum_reward = 0\n",
        "        discnt_rewards = []\n",
        "        self.reward_memory.reverse()\n",
        "        for r in self.reward_memory:\n",
        "            sum_reward = r + self.gamma*sum_reward\n",
        "            discnt_rewards.append(sum_reward)\n",
        "        discnt_rewards.reverse() \n",
        "        \n",
        "\n",
        "        for state,action,reward in zip(self.state_memory,self.action_memory,discnt_rewards):\n",
        "            with tf.GradientTape() as tape:\n",
        "                p = self.model(np.array([state]),training=True)\n",
        "                loss = self.calc_loss(p,action,reward)\n",
        "                grads = tape.gradient(loss,self.model.trainable_variables)\n",
        "                self.opt.apply_gradients(zip(grads,self.model.trainable_variables))\n",
        "\n",
        "        self.reward_memory = []\n",
        "        self.action_memory = []\n",
        "        self.state_memory = []\n",
        "\n",
        "    def calc_loss(self,prob,action,reward):\n",
        "        dist = tfp.distributions.Categorical(probs=prob, dtype=tf.float32)\n",
        "        log_prob = dist.log_prob(action)\n",
        "        loss = -log_prob*reward\n",
        "        return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TNMYevwsmWni"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "score_arr = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6CIvAKS9mOJ2",
        "outputId": "5e2c90eb-4751-4d57-a91c-5426070a210e"
      },
      "outputs": [],
      "source": [
        "env = gym.make('CartPole-v1')\n",
        "agent = Agent()\n",
        "num_episodes = 1000\n",
        "\n",
        "for i in range(num_episodes):\n",
        "    state = env.reset()\n",
        "    score = 0\n",
        "    rewards = []\n",
        "    states = []\n",
        "    actions = []\n",
        "    done = False\n",
        "    while not done:\n",
        "        action = agent.choose_action(state)\n",
        "        state_,reward,done,_ = env.step(action)\n",
        "        agent.store_reward(reward)\n",
        "        agent.store_state(state)\n",
        "        state = state_\n",
        "        score += reward\n",
        "        # env.render()\n",
        "        if done:\n",
        "            agent.learn()\n",
        "            score_arr.append(score)\n",
        "            if i % 50:\n",
        "                print(f'episode done: {i+1}\\t score recieved: {score}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t-bjAkcBmSIm"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(score_arr)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    },
    "vscode": {
      "interpreter": {
        "hash": "afb734500600fd355917ca529030176ea0ca205570884b88f2f6f7d791fd3fbe"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
